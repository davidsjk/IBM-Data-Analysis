#IMPORTING TOOLS (PANDAS, MATPLOTLIB, NUMPY, SEABORN, SKLEARN.Pipeline, SKLEARN.Preprocessing (polynomial features))


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,PolynomialFeatures
%matplotlib inline
from scipy import stats

#install the xlrd module:
!conda install -c anaconda xlrd --yes

#Load Exel
df_can = pd.read_excel('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DV0101EN/labs/Data_Files/Canada.xlsx',
                       sheet_name='Canada by Citizenship',
                       skiprows=range(20),
                       skipfooter=2)

#LOAD CSV

#load from online

file_name='https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/coursera/project/kc_house_data_NaN.csv'
df=pd.read_csv(file_name)


#Load a table without a header
df=pd.read_csv(file_name, header = None)

# add header to table without header
1# create header list
headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
"drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
"num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
"peak-rpm","city-mpg","highway-mpg","price"]

#Add header while creating DF
headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
         "peak-rpm","city-mpg","highway-mpg","price"]
df = pd.read_csv(filename, names = headers)

# add header to df
df.columns = headers



#SAVE CSV ( and others)

#download to local machine
df.to_csv("automobile.csv",index = False)




#DATA Wrangling


# Drop missing values along the column "price" as follows
df.dropna(subset=["price"], axis=0)

#Select the columns of a data frame
df[['length','compression-ratio']]

#Replace Missing Values with n
df = df.fillna(n) 

# replace "?" to NaN
df.replace("?", np.nan, inplace = True)

#Evaluate Missing Data
missing_data = df.isnull()
missing_data.head(5)

#Count missing values in each column
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("") 

#Calculate the average of the column
avg_norm_loss = df["normalized-losses"].astype("float").mean(axis=0)
print("Average of normalized-losses:", avg_norm_loss)

#Replace "NaN" by mean value in "normalized-losses" column
df["normalized-losses"].replace(np.nan, avg_norm_loss, inplace=True)

#replace the missing 'num-of-doors' values by the most frequent (‘four’ for this case)
df["num-of-doors"].replace(np.nan, "four", inplace=True)

#Drop whole row with NaN in "price" column
df.dropna(subset=["price"], axis=0, inplace=True)
## reset index, because we droped two rows
df.reset_index(drop=True, inplace=True)

#Convert data types to proper format
df[["bore", "stroke"]] = df[["bore", "stroke"]].astype("float")
df[["normalized-losses"]] = df[["normalized-losses"]].astype("int")
df[["price"]] = df[["price"]].astype("float")
df[["peak-rpm"]] = df[["peak-rpm"]].astype("float")

#Data Standardization
# #Convert mpg to L/100km by mathematical operation (235 divided by mpg)
df['city-L/100km'] = 235/df["city-mpg"]

#Data Normalization
## replace (original value) by (original value)/(maximum value)
df['length'] = df['length']/df['length'].max()
df['width'] = df['width']/df['width'].max()

#Binning
##Binning is a process of transforming continuous numerical variables into discrete categorical 'bins', for grouped analysis.
##Convert data to correct format
df["horsepower"]=df["horsepower"].astype(int, copy=True)
##Lets plot the histogram of horspower, to see what the distribution of horsepower looks like.
%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df["horsepower"])
## set x/y labels and plot title
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")
##Create BIN
bins = np.linspace(min(df["horsepower"]), max(df["horsepower"]), 4)
bins
##set group names:
group_names = ['Low', 'Medium', 'High']
##Cut
df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )
df[['horsepower','horsepower-binned']].head(20)
##Lets see the number of vehicles in each bin.
df["horsepower-binned"].value_counts()
##Plot Bin
%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["horsepower-binned"].value_counts())
## set x/y labels and plot title
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")
##Create histogram for Bin
%matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
a = (0,1,2)
## draw historgram of attribute "horsepower" with bins = 3
plt.pyplot.hist(df["horsepower"], bins = 3)
## set x/y labels and plot title
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")


#Indicator Variable
##(An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called 'dummies' because the numbers themselves don't have inherent meaning.
##get indicator variables and assign it to data frame "dummy_variable_1"
dummy_variable_1 = pd.get_dummies(df["fuel-type"])
dummy_variable_1.head()
##change column names for clarity
dummy_variable_1.rename(columns={'fuel-type-diesel':'gas', 'fuel-type-diesel':'diesel'}, inplace=True)
## merge data frame "df" and "dummy_variable_1" 
df = pd.concat([df, dummy_variable_1], axis=1)
##drop original column "fuel-type" from "df"
df.drop("fuel-type", axis = 1, inplace=True)

#create a Total Column
df_can['Total'] = df_can.sum(axis=1)

#Select Columns
##Method 1: Quick and easy, but only works if the column name does NOT have spaces or special characters.

    df.column_name 
        (returns series)
##Method 2: More robust, and can filter on multiple columns.

    df['column']  
        (returns series)
    df[['column 1', 'column 2']] 
        (returns dataframe)
        
#Select Rows
 df.loc[label]        
        ##filters by the labels of the index/column
  df.iloc[index]       
       # #filters by the positions of the index/column

Set a different Column as Index
df_can.set_index('Country', inplace=True)
# tip: The opposite of set is reset. So to reset the index, we can use df_can.reset_index()

# optional: to remove the name of the index
df_can.index.name = None






#DATA ANALYSIS

#Print entire dataframe
df

#Print the first n rows of dataframe
df.head(n)

#Print the last n rows of dataframe
df.tail(n)

#Find the names of the columns
df.columns

#Find data-type of each column
df.dtypes

#Get statistical summary of each column (Numeric type only)
df.describe()

#Describe all the columns in "df"
df.describe(include = "all")

#Describe columns with objects
df.describe(include=['object'])

#Describe select columns
df[['length','compression-ratio']].describe()

#Look at the info of "df"
df.info

$Get list of indicies 
df.index.values

#get the index and columns as lists
df_can.columns.tolist()
df_can.index.tolist()

print (type(df_can.columns.tolist()))
print (type(df_can.index.tolist()))

#Find the correlation between all variables
df.corr()

#Find correlation between certain columns
df[['bore','stroke' ,'compression-ratio','horsepower']].corr()

#Count the occurrence of variable in a column
df['drive-wheels'].value_counts()

#Make value count to a dataframe
df['drive-wheels'].value_counts().to_frame()

#Make/Save/Name Header/Name Index of a value count dataframe
## Make and Save
drive_wheels_counts = df['drive-wheels'].value_counts().to_frame()
##Rename Column
drive_wheels_counts.rename(columns={'drive-wheels': 'value_counts'}, inplace=True)
##Name index
drive_wheels_counts.index.name = 'drive-wheels'

#Calculate f the most common type:
df['num-of-doors'].value_counts().idxmax()

#Get list of unique variable
df['drive-wheels'].unique()

#Group By
##Make new DF with necessary columns
df_group_one = df[['drive-wheels','body-style','price']]
###Assign the group by column, and find the mean of each group.
df_group_one = df_group_one.groupby(['drive-wheels'],as_index=False).mean()
df_group_one

#Group By Multiple Variables
df_gptest = df[['drive-wheels','body-style','price']]
grouped_test1 = df_gptest.groupby(['drive-wheels','body-style'],as_index=False).mean()
## Convert to a Pivot table
grouped_pivot = grouped_test1.pivot(index='drive-wheels',columns='body-style')


#Calculate the Pearson Correlation Coefficient and P-value
from scipy import stats
pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])
print("The Pearson Correlation Coecient is", pearson_coef, " with a P-value of P =", p_value)


#ANOVA: Analysis of Variance
The Analysis of Variance (ANOVA) is a statistical method used to test whether there are significant
differences between the means of two or more groups. ANOVA returns two parameters:
F-test score: ANOVA assumes the means of all groups are the same, calculates how much the
actual means deviate from the assumption, and reports it as the F-test score. A larger score means
there is a larger difference between the means.
P-value: P-value tells how statistically significant is our calculated score value.
##Groupby different groups in the same variable
grouped_test2=df_gptest[['drive-wheels', 'price']].groupby(['drive-wheels'])
##We can obtain the values of the method group using the method "get_group".
grouped_test2.get_group('4wd')['price']
##ANOVA
f_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  
print( "ANOVA results: F=", f_val, ", P =", p_val)



#DATA VISUALIZATION

#Scatter Plot 
##(good way to visualize Continuous numerical variables ["int64" or "float64”])
sns.regplot(x="engine-size", y="price", data=df)

#Scatter Plot starts with Y = 0
sns.regplot(x="engine-size", y="price", data=df)
plt.ylim(0,)

#Box Plot
##(good way to visualize categorical variables [“object" or "int64”])
sns.boxplot(x="body-style", y="price", data=df)

#Heat Map
##(heatmap plots the target variable proportional to colour with respect to two variables)
plt.pcolor(grouped_pivot, cmap='RdBu')
plt.colorbar()
plt.show()
##Label names
row_labels = grouped_pivot.columns.levels[1]
col_labels = grouped_pivot.index
##Move ticks and labels to the center
ax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)
ax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)
##Insert labels
ax.set_xticklabels(row_labels, minor=False)
ax.set_yticklabels(col_labels, minor=False)
##Rotate label if too long
plt.xticks(rotation=90)
g.colorbar(im)
##Show
plt.show()


#Linear Regression
##Create the linear regression object
lm = LinearRegression()
lm
##Define Variables
X = df[['highway-mpg']]
Y = df['price']
##Fit linear Model
lm.fit(X,Y)
##Out put a prediction
Yhat=lm.predict(X)
Yhat[0:5] 
##Intercept (a)
lm.intercept_
##Slope(b)
lm.coef_

##Calculate R^2
lm.score(X,Y)

##FINAL estimated linear Model = Yhat = a + bX

#Regression Plot
##(Good for simple linear regressions. )
## import the visualization package: seaborn
import seaborn as sns
%matplotlib inline
##Enter chart
width = 12
height = 10
plt.figure(figsize=(width, height))
sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0,)

#Residual Plot
##(good way to visualize the variance of the data.If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data.)
##Code
width = 12
height = 10
plt.figure(figsize=(width, height))
sns.residplot(df['highway-mpg'], df['price'])
plt.show()



#Multiple Linear Regression
##Define Variable ‘X’
Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]
##Fit model
lm.fit(Z, df['price'])
##Intercept (a)
lm.intercept_
##Slope(b)
lm.coef_

# Multiple Linear Regression Plot
##First make a Prediction
Y_hat = lm.predict(Z)
## Code
plt.figure(figsize=(width, height))

ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values" , ax=ax1)

plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()



MODEL EVALUATION AND REFINEMENT

#Only use numeric Data in Df
df=df._get_numeric_data()

# Libraries for plotting 
%%capture
! pip install ipywidgets

from IPython.display import display
from IPython.html import widgets 
from IPython.display import display
from ipywidgets import interact, interactive, fixed, interact_manual

#Functions for Plotting

#Distribution Plot
def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):
    width = 12
    height = 10
    plt.figure(figsize=(width, height))

    ax1 = sns.distplot(RedFunction, hist=False, color="r", label=RedName)
    ax2 = sns.distplot(BlueFunction, hist=False, color="b", label=BlueName, ax=ax1)

    plt.title(Title)
    plt.xlabel('Price (in dollars)')
    plt.ylabel('Proportion of Cars')

    plt.show()
    plt.close()

#Poliynomial Plot
def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):
    width = 12
    height = 10
    plt.figure(figsize=(width, height))
    
    
    #training data 
    #testing data 
    # lr:  linear regression object 
    #poly_transform:  polynomial transformation object 
 
    xmax=max([xtrain.values.max(), xtest.values.max()])

    xmin=min([xtrain.values.min(), xtest.values.min()])

    x=np.arange(xmin, xmax, 0.1)

    plt.plot(xtrain, y_train, 'ro', label='Training Data')
    plt.plot(xtest, y_test, 'go', label='Test Data')
    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')
    plt.ylim([-10000, 60000])
    plt.ylabel('Price')
    plt.legend()


#Divide to Training and Testing
##place the target data price in a separate dataframe y:
y_data = df['price']
##drop price data in x data
x_data=df.drop('price',axis=1)
##Randomly split our data into training and testing data using the function train_test_split (adjust test size)
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)

print("number of test samples :", x_test.shape[0])
print("number of training samples:",x_train.shape[0])

#Testing a Linear Regression Model 
##Let's import LinearRegression from the module linear_model
from sklearn.linear_model import LinearRegression
##create a Linear Regression object:
lre=LinearRegression()
##we fit the model using the feature horsepower
lre.fit(x_train[['horsepower']], y_train)
##Calculate the R^2 on the test data
lre.score(x_test[['horsepower']], y_test)
##R^2 is much smaller using the test data
lre.score(x_train[['horsepower']], y_train)


#Cross-validation Score
##import model_selection from the module cross_val_score.
from sklearn.model_selection import cross_val_score
##We input the object, the feature in this case ' horsepower', the target data (y_data). The parameter 'cv' determines the number of folds; in this case 4
Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)
##default scoring is R^2; each element in the array has the average R^2 value in the fold:
Rcross
##calculate the average and standard deviation of our estimate
print("The mean of the folds are", Rcross.mean(), "and the standard deviation is" , Rcross.std())
##We can use negative squared error as a score by setting the parameter 'scoring' metric to 'neg_mean_squared_error'
-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')

##You can also use the function 'cross_val_predict' to predict the output. The function splits up the data into the specified number of folds, using one fold to get a prediction while the rest of the folds are used as test data. First import the function:
yhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)
yhat[0:5]



#Plotting Training and Testing (Actual and Prediction)
##Create Multiple linear regression objects
lr = LinearRegression()
lr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)
##Prediction using training data:
yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])
yhat_train[0:5]
##Prediction using test data:
yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])
yhat_test[0:5]

##Let's perform some model evaluation using our training and testing data separately. 
##First we import the seaborn and matplotlibb library for plotting.
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
##Let's examine the distribution of the predicted values of the training data.
Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'
DistributionPlot(y_train, yhat_train, "Actual Values (Train)", "Predicted Values (Train)", Title)
##Use test data
Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'
DistributionPlot(y_test,yhat_test,"Actual Values (Test)","Predicted Values (Test)",Title)

#Polynomial Regression
##Create Test and Training sets
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)
#perform a degree 5 polynomial transformation on the feature 'horse power'.
pr = PolynomialFeatures(degree=5)
x_train_pr = pr.fit_transform(x_train[['horsepower']])
x_test_pr = pr.fit_transform(x_test[['horsepower']])
pr
##Make a linear regression model and train it
poly = LinearRegression()
poly.fit(x_train_pr, y_train)
##We can see the output of our model using the method "predict." then assign the values to "yhat".
yhat = poly.predict(x_test_pr)
yhat[0:5]
##Let's take the first five predicted values and compare it to the actual targets.
print("Predicted values:", yhat[0:4])
print("True values:", y_test[0:4].values)
##display the training data, testing data, and the predicted function using PollyPlot
PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)
##R^2 of the training data:
poly.score(x_train_pr, y_train)
##R^2 of the test data:
poly.score(x_test_pr, y_test)
##Let's see how the R^2 changes on the test data for different order polynomials and plot the results:
Rsqu_test = []

order = [1, 2, 3, 4]
for n in order:
    pr = PolynomialFeatures(degree=n)
    
    x_train_pr = pr.fit_transform(x_train[['horsepower']])
    
    x_test_pr = pr.fit_transform(x_test[['horsepower']])    
    
    lr.fit(x_train_pr, y_train)
    
    Rsqu_test.append(lr.score(x_test_pr, y_test))

plt.plot(order, Rsqu_test)
plt.xlabel('order')
plt.ylabel('R^2')
plt.title('R^2 Using Test Data')
plt.text(3, 0.75, 'Maximum R^2 ')    



#Interactive Polynomial Order Test
def f(order, test_data):
    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)
    pr = PolynomialFeatures(degree=order)
    x_train_pr = pr.fit_transform(x_train[['horsepower']])
    x_test_pr = pr.fit_transform(x_test[['horsepower']])
    poly = LinearRegression()
    poly.fit(x_train_pr,y_train)
    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)

interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))


#Ridge Regression
#In this section, we will review Ridge Regression we will see how the parameter Alfa changes the model. Just a note here our test data will be used as validation data.
##Let's perform a degree two polynomial transformation on our data.
pr=PolynomialFeatures(degree=2)
x_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])
x_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])
##Let's import Ridge from the module linear models.
from sklearn.linear_model import Ridge
##Let's create a Ridge regression object, setting the regularization parameter to 0.1
RigeModel=Ridge(alpha=0.1)
##fit the model using the method fit.
RigeModel.fit(x_train_pr, y_train)
##Obtain Prediction
yhat = RigeModel.predict(x_test_pr)
##Let's compare the first five predicted samples to our test set
print('predicted:', yhat[0:4])
print('test set :', y_test[0:4].values)
##We select the value of Alfa that minimizes the test error, for example, we can use a for loop.
Rsqu_test = []
Rsqu_train = []
dummy1 = []
ALFA = 10 * np.array(range(0,1000))
for alfa in ALFA:
    RigeModel = Ridge(alpha=alfa) 
    RigeModel.fit(x_train_pr, y_train)
    Rsqu_test.append(RigeModel.score(x_test_pr, y_test))
    Rsqu_train.append(RigeModel.score(x_train_pr, y_train))
##We can plot out the value of R^2 for different Alphas
width = 12
height = 10
plt.figure(figsize=(width, height))

plt.plot(ALFA,Rsqu_test, label='validation data  ')
plt.plot(ALFA,Rsqu_train, 'r', label='training Data ')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend()



#Grid Search
##The term Alfa is a hyperparameter, sklearn has the class GridSearchCV to make the process of finding the best hyperparameter simpler.
##Let's import GridSearchCV from the module model_selection.
from sklearn.model_selection import GridSearchCV
##We create a dictionary of parameter values:
parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]
parameters1
##Create a ridge regions object:
RR=Ridge()
RR
##Create a ridge grid search object
Grid1 = GridSearchCV(RR, parameters1,cv=4)
##Fit the model
Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)
##The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:
BestRR=Grid1.best_estimator_
BestRR
##We now test our model on the test data
BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)
